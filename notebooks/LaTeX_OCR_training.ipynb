{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtR1GhYwnLnu"
      },
      "source": [
        "# Train a LaTeX OCR model\n",
        "In this brief notebook I show how you can finetune/train an OCR model.\n",
        "\n",
        "I've opted to mix in handwritten data into the regular pdf LaTeX images. For that I started out with the released pretrained model and continued training on the slightly larger corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r396ah-Q3EQc"
      },
      "outputs": [],
      "source": [
        "!pip install pix2tex[train] -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZ4PLwkb3RIs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "!mkdir -p LaTeX-OCR\n",
        "os.chdir('LaTeX-OCR')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUsTlxXV3Mot"
      },
      "outputs": [],
      "source": [
        "!pip install gpustat -q\n",
        "!pip install opencv-python-headless==4.1.2.30 -U -q\n",
        "!pip install --upgrade --no-cache-dir gdown -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhLzh5vyaCaL"
      },
      "outputs": [],
      "source": [
        "# check what GPU we have\n",
        "!gpustat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAz37dDU21zu"
      },
      "outputs": [],
      "source": [
        "!mkdir -p dataset/data\n",
        "!mkdir images\n",
        "# Google Drive ids\n",
        "# handwritten: 13vjxGYrFCuYnwgDIUqkxsNGKk__D_sOM\n",
        "# pdf - images: 176PKaCUDWmTJdQwc-OfkO0y8t4gLsIvQ\n",
        "# pdf - math: 1QUjX6PFWPa-HBWdcY-7bA5TRVUnbyS1D\n",
        "!gdown -O dataset/data/crohme.zip --id 13vjxGYrFCuYnwgDIUqkxsNGKk__D_sOM\n",
        "!gdown -O dataset/data/pdf.zip --id 176PKaCUDWmTJdQwc-OfkO0y8t4gLsIvQ\n",
        "!gdown -O dataset/data/pdfmath.txt --id 1QUjX6PFWPa-HBWdcY-7bA5TRVUnbyS1D\n",
        "os.chdir('dataset/data')\n",
        "!unzip -q crohme.zip \n",
        "!unzip -q pdf.zip \n",
        "# split handwritten data into val set and train set\n",
        "os.chdir('images')\n",
        "!mkdir ../valimages\n",
        "!ls | shuf -n 1000 | xargs -i mv {} ../valimages\n",
        "os.chdir('../../..')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BMuIqRIqG-8"
      },
      "source": [
        "Now we generate the datasets. We can string multiple datasets together to get one large lookup table. The only thing saved in these pkl files are image sizes, image location and the ground truth latex code. That way we can serve batches of images with the same dimensionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JebcEarl-g6"
      },
      "outputs": [],
      "source": [
        "!python -m pix2tex.dataset.dataset -i dataset/data/images dataset/data/train -e dataset/data/CROHME_math.txt dataset/data/pdfmath.txt -o dataset/data/train.pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_Orutb37xHD"
      },
      "outputs": [],
      "source": [
        "!python -m pix2tex.dataset.dataset -i dataset/data/valimages dataset/data/val -e dataset/data/CROHME_math.txt dataset/data/pdfmath.txt -o dataset/data/val.pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3iOyEEBbw58"
      },
      "outputs": [],
      "source": [
        "# download the weights we want to fine tune\n",
        "!curl -L -o weights.pth https://github.com/lukas-blecher/LaTeX-OCR/releases/download/v0.0.1/weights.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vow2NnpHmWt0"
      },
      "outputs": [],
      "source": [
        "# If using wandb\n",
        "!pip install -q wandb \n",
        "# you can cancel this if you don't wan't to use it or don't have a W&B acc.\n",
        "#!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnsNCLp84QSY"
      },
      "outputs": [],
      "source": [
        "# generate colab specific config (set 'debug' to true if wandb is not used)\n",
        "!echo {backbone_layers: [2, 3, 7], betas: [0.9, 0.999], batchsize: 10, bos_token: 1, channels: 1, data: dataset/data/train.pkl, debug: true, decoder_args: {'attn_on_attn': true, 'cross_attend': true, 'ff_glu': true, 'rel_pos_bias': false, 'use_scalenorm': false}, dim: 256, encoder_depth: 4, eos_token: 2, epochs: 50, gamma: 0.9995, heads: 8, id: null, load_chkpt: 'weights.pth', lr: 0.001, lr_step: 30, max_height: 192, max_seq_len: 512, max_width: 672, min_height: 32, min_width: 32, model_path: checkpoints, name: mixed, num_layers: 4, num_tokens: 8000, optimizer: Adam, output_path: outputs, pad: false, pad_token: 0, patch_size: 16, sample_freq: 2000, save_freq: 1, scheduler: StepLR, seed: 42, temperature: 0.2, test_samples: 5, testbatchsize: 20, tokenizer: dataset/tokenizer.json, valbatches: 100, valdata: dataset/data/val.pkl} > colab.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8NU5j2k3z36"
      },
      "outputs": [],
      "source": [
        "!python -m pix2tex.train --config colab.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "g3DU9KxubWgq"
      },
      "outputs": [],
      "source": [
        "!pip install gdown -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Access denied with the following error:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Admins\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\gdown\\cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "\n",
            " \tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/uc?id=1l_lbvsUnUKApUuqnYeGTpZbb5PCBrCGb \n",
            "\n"
          ]
        }
      ],
      "source": [
        "id = '1l_lbvsUnUKApUuqnYeGTpZbb5PCBrCGb'\n",
        "\n",
        "# download the weights we want to fine tune\n",
        "!gdown -O weights.pth --id 1l_lbvsUnUKApUuqnYeGTpZbb5PCBrCGb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Admins\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "ename": "NotImplementedError",
          "evalue": "Encoder structure \"convnext\" not supported.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32md:\\LaTeX_OCR\\LaTeX-ConvNeXt\\notebooks\\LaTeX_OCR_training.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/LaTeX_OCR/LaTeX-ConvNeXt/notebooks/LaTeX_OCR_training.ipynb#X21sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m args \u001b[39m=\u001b[39m Munch(config)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/LaTeX_OCR/LaTeX-ConvNeXt/notebooks/LaTeX_OCR_training.ipynb#X21sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# print(args)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/LaTeX_OCR/LaTeX-ConvNeXt/notebooks/LaTeX_OCR_training.ipynb#X21sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m model \u001b[39m=\u001b[39m get_model(args)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/LaTeX_OCR/LaTeX-ConvNeXt/notebooks/LaTeX_OCR_training.ipynb#X21sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# #get total parameters of the model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/LaTeX_OCR/LaTeX-ConvNeXt/notebooks/LaTeX_OCR_training.ipynb#X21sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# total_params = sum(p.numel() for p in model.parameters())\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/LaTeX_OCR/LaTeX-ConvNeXt/notebooks/LaTeX_OCR_training.ipynb#X21sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\Admins\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pix2tex-0.0.0-py3.8.egg\\pix2tex\\models\\utils.py:59\u001b[0m, in \u001b[0;36mget_model\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     57\u001b[0m     encoder \u001b[39m=\u001b[39m hybrid\u001b[39m.\u001b[39mget_encoder(args)\n\u001b[0;32m     58\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mEncoder structure \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m not supported.\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m args\u001b[39m.\u001b[39mencoder_structure)\n\u001b[0;32m     60\u001b[0m decoder \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mget_decoder(args)\n\u001b[0;32m     61\u001b[0m encoder\u001b[39m.\u001b[39mto(args\u001b[39m.\u001b[39mdevice)\n",
            "\u001b[1;31mNotImplementedError\u001b[0m: Encoder structure \"convnext\" not supported."
          ]
        }
      ],
      "source": [
        "import os\n",
        "import argparse\n",
        "import logging\n",
        "import yaml\n",
        "\n",
        "import torch\n",
        "from munch import Munch\n",
        "import torch.nn as nn\n",
        "from pix2tex.models import get_model\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "config_path = \"D:\\LaTeX_OCR\\LaTeX-ConvNeXt\\checkpoints_convnext\\convnext_teacher_new\\config.yaml\"\n",
        "with open(config_path, \"r\") as f:\n",
        "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
        "args = Munch(config)\n",
        "\n",
        "# print(args)\n",
        "model = get_model(args)\n",
        "\n",
        "# #get total parameters of the model\n",
        "# total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "num_epochs = 5\n",
        "step_per_epoch = round(int(13924/32))\n",
        "tt_steps = num_epochs*step_per_epoch\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.05)\n",
        "scheduler = OneCycleLR(optimizer, \n",
        "                       max_lr=0.01,\n",
        "                       epochs = 5,\n",
        "                       pct_start = 0.3,\n",
        "                       steps_per_epoch = step_per_epoch,\n",
        "                       total_steps=tt_steps,\n",
        "                       anneal_strategy=\"cos\", \n",
        "                       div_factor=15, \n",
        "                       final_div_factor=100\n",
        "                      )\n",
        "\n",
        "lr_list = []\n",
        "for i in range(5):\n",
        "    for j in range(step_per_epoch):\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        lr_list.append(scheduler.get_last_lr())\n",
        "\n",
        "plt.plot(lr_list)\n",
        "plt.xlabel('Iterations')\n",
        "plt.yscale('logit')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "LaTeX-OCR training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "25e4cfa5f389ddf8aba8c35b4583df43c1d17ebd7afb7675daa269e3c8e4befb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
